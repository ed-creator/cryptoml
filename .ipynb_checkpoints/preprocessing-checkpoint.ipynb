{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv, DataFrame, concat\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, Dropout, LSTM\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to add a prefix to all column headings\n",
    "def rename_columns(prefix, dataset):\n",
    "    for heading in (dataset.columns):\n",
    "        dataset.rename(columns = {heading : (prefix + '_' + heading)},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the row full data starts\n",
    "def get_shortest_feature(dataset):\n",
    "    lenghts = []\n",
    "    for col in ['txVolume(USD)', 'txCount', 'marketcap(USD)', 'price(USD)',\n",
    "           'exchangeVolume(USD)', 'activeAddresses', 'medianTxValue(USD)']:\n",
    "        lenghts.append(len(dataset[dataset[col] >0]))\n",
    "    return min(lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "def get_float_headings_list(dataset):\n",
    "    float_headings = []\n",
    "    float_columns = dataset.loc[:0, dataset.dtypes == float].columns\n",
    "    for index, heading in enumerate(float_columns):\n",
    "        float_headings.append(dataset.columns.get_loc(float_columns[index]))\n",
    "    return float_headings\n",
    "\n",
    "def plot_timeseries_graphs(dataset, name=''):\n",
    "    values = dataset.values\n",
    "    # specify columns to plot\n",
    "    groups = get_float_headings_list(dataset)\n",
    "    i = 1\n",
    "    # plot each column\n",
    "    pyplot.figure()\n",
    "    for group in groups:\n",
    "        pyplot.subplot(len(groups), 1, i)\n",
    "        pyplot.plot(values[:, group])\n",
    "        pyplot.title(name + ' ' + dataset.columns[group], y=0.5, loc='right')\n",
    "        i += 1\n",
    "        \n",
    "    return pyplot.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to slice a base dataset based to match with the dates of a dataset to be analyzed\n",
    "def slice_base_asset_data(base_dataset, sliced_dataset, date_column_name='date'):\n",
    "    valid_dates = sliced_dataset['date'].values\n",
    "    sliced_base_data = base_dataset.loc[base_dataset['eth_date'].isin(valid_dates)]\n",
    "    return sliced_base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to merge two datasets by date\n",
    "def merge_frames(dataset, base_dataset, dataset_column_name='date', base_dataset_column_name='eth_date'):\n",
    "    x =pd.merge(dataset.rename(columns={dataset_column_name:'date'}), \n",
    "                base_dataset.rename(columns={base_dataset_column_name:'date'}), \n",
    "                on='date', how='left')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to replace 0/NaN values, First row is deleted if Nans exist, remaining missing values interploated\n",
    "def replace_bad_values(dataset):\n",
    "    dataset = dataset.copy().replace(0,np.nan)\n",
    "    if dataset.loc[0].isnull().sum() > 0:\n",
    "        dataset.drop(0, inplace=True)\n",
    "        dataset.reset_index(inplace=True)\n",
    "    return dataset.interpolate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to establish if ETH or REP performed better for the time period. This is the label\n",
    "def generate_label(price_percent_change, eth_price_percent_change):\n",
    "    if price_percent_change > eth_price_percent_change:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate labels column\n",
    "def generate_labels(dataset):\n",
    "    performance_vs_eth = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        x = generate_label(row[\"price_percent_inrcease(USD)\"], row[\"eth_price_percent_inrcease(USD)\"])\n",
    "        performance_vs_eth.append(x)\n",
    "    return performance_vs_eth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=10, n_out=6, dropnan=False):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessing():\n",
    "        \n",
    "    def __init__(self, erc20_names = ['ae','bat','fun','gno','gnt','loom','omg','rep','salt','snt','zrx']):\n",
    "        self.erc20_names = erc20_names # List of all erc20 tokens to be evaluated\n",
    "        self.raw_data = {} # raw DataFrames from csv\n",
    "        self.merged_data = {} # erc20 DataFrames with missing data removed & join with eth dataset on date\n",
    "        self.clean_data = {} # interpolated merged_data set to remove nan values (some first rows removed due to NaN values)\n",
    "        self.normalized_data = {} # normalized data, including corresponding dates & col_names\n",
    "        self.full_data = {}\n",
    "        self.lstm_sets = {} #dataset converted to lstm compatible\n",
    "        self.train_test_sets = {}\n",
    "        \n",
    "        \n",
    "        self.train_X = None\n",
    "        self.test_X = None\n",
    "        self.train_y = None\n",
    "        self.test_y = None\n",
    "        self.history = None\n",
    "        \n",
    "        self.FEATURES_TO_IGNORE = ['date']\n",
    "\n",
    "        self.get_raw_datasets(self.erc20_names)\n",
    "        \n",
    "        \n",
    "    def get_raw_datasets(self, erc20_names):\n",
    "        eth_dataset=read_csv('eth.csv', index_col=False)\n",
    "        rename_columns('eth', eth_dataset)\n",
    "        self.raw_data['eth'] = eth_dataset\n",
    "        for erc20 in erc20_names:\n",
    "            csv = erc20 + '.csv'            \n",
    "            dataset=read_csv(csv, index_col=False)\n",
    "            self.raw_data[erc20] = dataset\n",
    "            \n",
    "    def merge_data(self):\n",
    "        for erc20 in self.erc20_names:\n",
    "            dataset = self.raw_data[erc20].copy().fillna(0)\n",
    "            eth_dataset = self.raw_data['eth'].copy()\n",
    "            \n",
    "            #get the number of rows that have all valid data & slice datasets to only include this\n",
    "            valid_record_count = len(dataset) - get_shortest_feature(dataset) \n",
    "            sliced_data = dataset.iloc[valid_record_count:]\n",
    "            sliced_eth_data = slice_base_asset_data(eth_dataset,sliced_data,'eth_date')\n",
    "            \n",
    "            #sanity check sliced datasets are the same shape\n",
    "            print('sliced eth data shape {}'.format(sliced_eth_data.shape))\n",
    "            print('sliced {} data shape {}'.format(erc20, sliced_data.shape))\n",
    "            \n",
    "            #merge erc20 data & eth data (join on date)\n",
    "            merged_data = merge_frames(sliced_data,eth_dataset)\n",
    "            self.merged_data[erc20] = merged_data.copy()\n",
    "            \n",
    "    def remove_nans(self):\n",
    "        for erc20 in self.erc20_names:\n",
    "            merged_data = replace_bad_values(self.merged_data[erc20].copy())            \n",
    "            #Repeat Sanity Check for missing values\n",
    "            print('{} NaN Cells: {}'.format(erc20, merged_data.isnull().values.sum()))\n",
    "            self.clean_data[erc20] = merged_data\n",
    "            \n",
    "    def populate_infered_data(self):\n",
    "        for erc20 in self.erc20_names:\n",
    "            \n",
    "            dataset = self.clean_data[erc20].copy()\n",
    "            \n",
    "            #add nan filled columns\n",
    "            dataset.insert(5, 'price_percent_inrcease(USD)', np.nan, allow_duplicates=False)\n",
    "            dataset.insert(5, 'price_inrcease(USD)', np.nan, allow_duplicates=False)\n",
    "            dataset.insert(15, 'eth_price_percent_inrcease(USD)', np.nan, allow_duplicates=False)\n",
    "            \n",
    "            #generate difference from previous day's price\n",
    "            dataset['price_inrcease(USD)'] = dataset['price(USD)'].diff()\n",
    "            dataset['eth_price_inrcease(USD)'] = dataset['eth_price(USD)'].diff()\n",
    "\n",
    "            #generate percentage difference from previous day's price\n",
    "            dataset['price_percent_inrcease(USD)'] = dataset['price(USD)'].pct_change()\n",
    "            dataset['eth_price_percent_inrcease(USD)'] = dataset['eth_price(USD)'].pct_change()\n",
    "\n",
    "            #Fill labels column, uses the next days price data to infer the target for the current day\n",
    "            dataset['current_performance_vs_eth'] = generate_labels(dataset)\n",
    "            dataset['next_day_performance_vs_eth'] = dataset['current_performance_vs_eth'].shift(-1)\n",
    "            \n",
    "            #NOTE: WOULD BE COOL TO ADD A HEAT CHART FOR WHAT DAYS ALL ASSETS OUTPERFORMED ETH\n",
    "            print('{} outperformed ETH on {} / {} days'.format(erc20, dataset['current_performance_vs_eth'].sum(), dataset['current_performance_vs_eth'].count()))\n",
    "            print(dataset['current_performance_vs_eth'].sum() / dataset['current_performance_vs_eth'].count() * 100,  '% of the time')\n",
    "\n",
    "            self.full_data[erc20] = dataset[1:-1]\n",
    "\n",
    "    #normalize features extract dates for future reference\n",
    "    def normalize_features(self, FEATURES_TO_IGNORE = None, ):\n",
    "        \n",
    "        if FEATURES_TO_IGNORE == None:\n",
    "            FEATURES_TO_IGNORE = self.FEATURES_TO_IGNORE\n",
    "            \n",
    "        target_col = ['next_day_performance_vs_eth'] \n",
    "        for erc20 in self.erc20_names:\n",
    "            \n",
    "            values = self.full_data[erc20].copy().drop(FEATURES_TO_IGNORE,axis=1)\n",
    "            values = values.drop(target_col, axis=1)\n",
    "            \n",
    "            col_names = values.columns.values\n",
    "            col_name_lookup = DataFrame(data = values.columns.values)\n",
    "            \n",
    "            col_name_lookup['Var_num'] = col_name_lookup.index + 1\n",
    "\n",
    "            dates = self.full_data[erc20].copy()['date'] \n",
    "            \n",
    "            targets = self.full_data[erc20].copy()['next_day_performance_vs_eth'].tolist()        \n",
    "            \n",
    "            values = values.astype('float32')\n",
    "\n",
    "            # normalize features\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            scaled = scaler.fit_transform(values)\n",
    "            \n",
    "            self.normalized_data[erc20] = {'features' : scaled, 'targets' : targets, 'dates' :  dates, 'col_names' : col_name_lookup}\n",
    "    \n",
    "    #fuction to convert features to be lstm compatible (basically adding cols from previous dates)\n",
    "    def series_to_supervised(self):\n",
    "        for erc20 in self.erc20_names:\n",
    "            reframed = series_to_supervised(self.normalized_data[erc20]['features'].copy(), 1, 1)\n",
    "            \n",
    "            reframed = reframed.copy().drop(0)\n",
    "        \n",
    "            self.lstm_sets[erc20] = {'features' : reframed, \n",
    "                                      'targets' : self.normalized_data[erc20]['targets'].copy()[1:],\n",
    "                                      'dates' : self.normalized_data[erc20]['dates'].copy()[1:],\n",
    "                                      'col_names' : self.normalized_data[erc20]['col_names'].copy()\n",
    "                                     }\n",
    "    def build_datasets(self):\n",
    "        self.merge_data()\n",
    "        self.remove_nans()\n",
    "        self.populate_infered_data()\n",
    "        self.normalize_features()\n",
    "        self.series_to_supervised()\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(40,input_shape=(self.train_X.shape[1], self.train_X.shape[2])))\n",
    "        # model.add(Flatten())\n",
    "        self.model.add(Dense(25))\n",
    "        self.model.add(Dropout(0.8))\n",
    "        self.model.add(Dense(25))\n",
    "        self.model.add(Dropout(0.8))\n",
    "\n",
    "        self.model.add(Dense(25))\n",
    "        self.model.add(Dropout(0.8))\n",
    "\n",
    "        self.model.add(Dense(25))\n",
    "\n",
    "        \n",
    "        self.model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            \n",
    "        return \"build complete\"\n",
    "    \n",
    "    def set_train_test_values(self, n_train_days=250):\n",
    "        for erc20 in self.erc20_names:\n",
    "            values = self.lstm_sets[erc20]['features'].values\n",
    "            targets = self.lstm_sets[erc20]['targets']\n",
    "            \n",
    "            train_X = values[:n_train_days, :]\n",
    "            test_X = values[n_train_days:, :]\n",
    "            \n",
    "#             self.train_test_sets[erc20]['train_y'] = targets[:n_train_days]\n",
    "#             self.train_test_sets[erc20]['test_y'] = targets[n_train_days:]\n",
    "\n",
    "            # split into input and outputs\n",
    "#             train_X, train_y = train[:, :]\n",
    "#             print(len(self.train_X), len(self.train_y))\n",
    "#             test_X, test_y = test[:, :]\n",
    "#             print(len(self.test_X), len(self.test_y))\n",
    "\n",
    "            # reshape input to be 3D [samples, timesteps, features]\n",
    "#             self.train_test_sets[erc20]['train_X'] = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "#             self.train_test_sets[erc20]['test_X'] = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "            self.train_test_sets[erc20] = {'train_y' : targets[:n_train_days], 'test_y' : targets[n_train_days:], \n",
    "                                           'train_X' : train_X.reshape((train_X.shape[0], 1, train_X.shape[1])) ,\n",
    "                                           'test_X' : test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "                                          }\n",
    "            \n",
    "        #use self.final_sets.get(rep) as the input variable CHANGE FUNCTION NAME\n",
    "    def set_train_test_values2(self, dataset, n_train_days=250):\n",
    "            values = dataset.values\n",
    "            train = values[:n_train_days, :]\n",
    "            test = values[n_train_days:, :]\n",
    "\n",
    "            # split into input and outputs\n",
    "            self.train_X, self.train_y = train[:, :-1], train[:, -1]\n",
    "            print(len(self.train_X), len(self.train_y))\n",
    "            self.test_X, self.test_y = test[:, :-1], test[:, -1]\n",
    "            print(len(self.test_X), len(self.test_y))\n",
    "\n",
    "            # reshape input to be 3D [samples, timesteps, features]\n",
    "            self.train_X = self.train_X.reshape((self.train_X.shape[0], 1, self.train_X.shape[1]))\n",
    "            self.test_X = self.test_X.reshape((self.test_X.shape[0], 1, self.test_X.shape[1]))\n",
    "            print(self.train_X.shape, self.train_y.shape, self.test_X.shape, self.test_y.shape)\n",
    "    \n",
    "    def set_eval_values(self, test_days=None):\n",
    "        for erc20 in self.final_sets:\n",
    "            \n",
    "            values = self.final_sets.get(erc20).values\n",
    "            days = len(values)\n",
    "            if test_days == None:\n",
    "                test_days = len(self.test_y)\n",
    "        \n",
    "            n_train_days = days - test_days \n",
    "            train = values[:n_train_days, :]\n",
    "            test = values[n_train_days:, :]\n",
    "\n",
    "            # split into input and outputs\n",
    "            train_X, train_y = train[:, :-1], train[:, -1]\n",
    "            print(len(train_X), len(train_y))\n",
    "            test_X, test_y = test[:, :-1], test[:, -1]\n",
    "            print(len(test_X), len(test_y))\n",
    "\n",
    "            # reshape input to be 3D [samples, timesteps, features]\n",
    "            self.eval_sets[erc20] = {'test_X' : test_X.reshape((test_X.shape[0], 1, test_X.shape[1])), 'test_y' : test_y} \n",
    "        \n",
    "    def fit_model(self):        \n",
    "        self.history = self.model.fit(self.train_X, self.train_y, epochs=400, batch_size=50, validation_data=(self.test_X, self.test_y), verbose=2, shuffle=False)\n",
    "        # plot history\n",
    "        pyplot.plot(self.history.history['loss'], label='train')\n",
    "        pyplot.plot(self.history.history['val_loss'], label='test')\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "\n",
    "        pyplot.plot(self.history.history['acc'], label='train')\n",
    "        pyplot.plot(self.history.history['val_acc'], label='test')\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataProcessing = DataProcessing(erc20_names=['ae','bat','fun','gno','gnt','rep'])\n",
    "DataProcessing = DataProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sliced eth data shape (312, 16)\n",
      "sliced ae data shape (312, 8)\n",
      "sliced eth data shape (411, 16)\n",
      "sliced bat data shape (411, 8)\n",
      "sliced eth data shape (369, 16)\n",
      "sliced fun data shape (369, 8)\n",
      "sliced eth data shape (441, 16)\n",
      "sliced gno data shape (441, 8)\n",
      "sliced eth data shape (595, 16)\n",
      "sliced gnt data shape (595, 8)\n",
      "sliced eth data shape (119, 16)\n",
      "sliced loom data shape (119, 8)\n",
      "sliced eth data shape (367, 16)\n",
      "sliced omg data shape (367, 8)\n",
      "sliced eth data shape (346, 16)\n",
      "sliced rep data shape (346, 8)\n",
      "sliced eth data shape (292, 16)\n",
      "sliced salt data shape (292, 8)\n",
      "sliced eth data shape (384, 16)\n",
      "sliced snt data shape (384, 8)\n",
      "sliced eth data shape (336, 16)\n",
      "sliced zrx data shape (336, 8)\n",
      "ae NaN Cells: 0\n",
      "bat NaN Cells: 0\n",
      "fun NaN Cells: 0\n",
      "gno NaN Cells: 0\n",
      "gnt NaN Cells: 0\n",
      "loom NaN Cells: 0\n",
      "omg NaN Cells: 0\n",
      "rep NaN Cells: 0\n",
      "salt NaN Cells: 0\n",
      "snt NaN Cells: 0\n",
      "zrx NaN Cells: 0\n",
      "ae outperformed ETH on 145 / 312 days\n",
      "46.47435897435898 % of the time\n",
      "bat outperformed ETH on 190 / 411 days\n",
      "46.228710462287104 % of the time\n",
      "fun outperformed ETH on 160 / 368 days\n",
      "43.47826086956522 % of the time\n",
      "gno outperformed ETH on 183 / 441 days\n",
      "41.49659863945578 % of the time\n",
      "gnt outperformed ETH on 257 / 595 days\n",
      "43.19327731092437 % of the time\n",
      "loom outperformed ETH on 53 / 119 days\n",
      "44.537815126050425 % of the time\n",
      "omg outperformed ETH on 161 / 367 days\n",
      "43.869209809264305 % of the time\n",
      "rep outperformed ETH on 155 / 346 days\n",
      "44.797687861271676 % of the time\n",
      "salt outperformed ETH on 120 / 292 days\n",
      "41.0958904109589 % of the time\n",
      "snt outperformed ETH on 158 / 384 days\n",
      "41.14583333333333 % of the time\n",
      "zrx outperformed ETH on 152 / 336 days\n",
      "45.23809523809524 % of the time\n"
     ]
    }
   ],
   "source": [
    "DataProcessing.build_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProcessing.set_train_test_values(n_train_days=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae\n",
      "9\n",
      "9\n",
      "300\n",
      "300\n",
      "3.0\n",
      "bat\n",
      "108\n",
      "108\n",
      "300\n",
      "300\n",
      "36.0\n",
      "fun\n",
      "65\n",
      "65\n",
      "300\n",
      "300\n",
      "21.666666666666668\n",
      "gno\n",
      "138\n",
      "138\n",
      "300\n",
      "300\n",
      "46.0\n",
      "gnt\n",
      "292\n",
      "292\n",
      "300\n",
      "300\n",
      "97.33333333333334\n",
      "loom\n",
      "0\n",
      "0\n",
      "116\n",
      "116\n",
      "0.0\n",
      "omg\n",
      "64\n",
      "64\n",
      "300\n",
      "300\n",
      "21.333333333333336\n",
      "rep\n",
      "43\n",
      "43\n",
      "300\n",
      "300\n",
      "14.333333333333334\n",
      "salt\n",
      "0\n",
      "0\n",
      "289\n",
      "289\n",
      "0.0\n",
      "snt\n",
      "81\n",
      "81\n",
      "300\n",
      "300\n",
      "27.0\n",
      "zrx\n",
      "33\n",
      "33\n",
      "300\n",
      "300\n",
      "11.0\n"
     ]
    }
   ],
   "source": [
    "for erc20 in DataProcessing.erc20_names:\n",
    "    print(erc20)\n",
    "    print(len(DataProcessing.train_test_sets[erc20]['test_X']))\n",
    "    print(len((DataProcessing.train_test_sets[erc20]['test_y'])))\n",
    "    print(len((DataProcessing.train_test_sets[erc20]['train_X'])))\n",
    "    print(len(DataProcessing.train_test_sets[erc20]['train_y']))\n",
    "    print((len(DataProcessing.train_test_sets[erc20]['test_y']) /len(DataProcessing.train_test_sets[erc20]['train_y']) ) *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProcessing.populate_infered_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProcessing.normalize_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProcessing.series_to_supervised()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProcessing.full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.defs.vis as vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.sns_labels_heatmap(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plotly_labels_heatmap(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProcessing.set_train_test_values(DataProcessing.final_sets.get('rep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_day_performance_vs_eth_omg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProcessing.set_eval_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProcessing.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProcessing.test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DataProcessing.merged_data['fun'].loc[0].isnull().sum() > 0:\n",
    "    print (\"yes\")\n",
    "else:\n",
    "    print (\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_bad_valuess(dataset):\n",
    "    dataset = dataset.copy().replace(0,np.nan)\n",
    "    if dataset.loc[0].isnull().sum() > 0:\n",
    "        dataset.drop(0, inplace=True)\n",
    "        dataset.reset_index(inplace=True)\n",
    "    return dataset.interpolate()\n",
    "\n",
    "replace_bad_valuess(DataProcessing.merged_data['fun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "train_x = scaled[:200, :]\n",
    "test_x = scaled[200:, :]\n",
    "\n",
    "train_y = targets.tolist()[:200]\n",
    "test_y = targets.tolist()[200:]\n",
    "\n",
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (clf.decision_path([train_x[1],train_x[2]], check_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
