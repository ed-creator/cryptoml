{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, Dropout, LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to add a prefix to all column headings\n",
    "def rename_columns(prefix, dataset):\n",
    "    for heading in (dataset.columns):\n",
    "        dataset.rename(columns = {heading : (prefix + '_' + heading)},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the row full data starts\n",
    "def get_shortest_feature(dataset):\n",
    "    lenghts = []\n",
    "    for col in ['txVolume(USD)', 'txCount', 'marketcap(USD)', 'price(USD)',\n",
    "           'exchangeVolume(USD)', 'activeAddresses', 'medianTxValue(USD)']:\n",
    "        lenghts.append(len(dataset[dataset[col] >0]))\n",
    "    return min(lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "def get_float_headings_list(dataset):\n",
    "    float_headings = []\n",
    "    float_columns = dataset.loc[:0, dataset.dtypes == float].columns\n",
    "    for index, heading in enumerate(float_columns):\n",
    "        float_headings.append(dataset.columns.get_loc(float_columns[index]))\n",
    "    return float_headings\n",
    "\n",
    "def plot_timeseries_graphs(dataset, name=''):\n",
    "    values = dataset.values\n",
    "    # specify columns to plot\n",
    "    groups = get_float_headings_list(dataset)\n",
    "    i = 1\n",
    "    # plot each column\n",
    "    pyplot.figure()\n",
    "    for group in groups:\n",
    "        pyplot.subplot(len(groups), 1, i)\n",
    "        pyplot.plot(values[:, group])\n",
    "        pyplot.title(name + ' ' + dataset.columns[group], y=0.5, loc='right')\n",
    "        i += 1\n",
    "        \n",
    "    return pyplot.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to slice a base dataset based to match with the dates of a dataset to be analyzed\n",
    "def slice_base_asset_data(base_dataset, sliced_dataset, date_column_name='date'):\n",
    "    valid_dates = sliced_dataset['date'].values\n",
    "    sliced_base_data = base_dataset.loc[base_dataset['eth_date'].isin(valid_dates)]\n",
    "    return sliced_base_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the above data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to merge two datasets by date\n",
    "def merge_frames(dataset, base_dataset, dataset_column_name='date', base_dataset_column_name='eth_date'):\n",
    "    x =pd.merge(dataset.rename(columns={dataset_column_name:'date'}), \n",
    "                base_dataset.rename(columns={base_dataset_column_name:'date'}), \n",
    "                on='date', how='left')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe Problem with having empty cells in time series data\n",
    "\n",
    "As A solution I decided to use the average of the previous & subsequent cell as a replacement. This is not perfect but will hopefully not affect the results significantly given the very low number of 0 value cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to replace 0/NaN values, First row is deleted if Nans exist, remaining missing values interploated\n",
    "def replace_bad_values(dataset):\n",
    "    dataset = dataset.copy().replace(0,np.nan)\n",
    "    if dataset.loc[0].isnull().sum() > 0:\n",
    "        dataset.drop(0, inplace=True)\n",
    "        dataset.reset_index(inplace=True)\n",
    "    return dataset.interpolate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in Additional Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to establish if ETH or REP performed better for the time period. This is the label\n",
    "def generate_label(price_percent_change, eth_price_percent_change):\n",
    "    if price_percent_change > eth_price_percent_change:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate labels column\n",
    "def generate_labels(dataset):\n",
    "    performance_vs_eth = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        x = generate_label(row[\"price_percent_inrcease(USD)\"], row[\"eth_price_percent_inrcease(USD)\"])\n",
    "        performance_vs_eth.append(x)\n",
    "    return performance_vs_eth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=10, n_out=6, dropnan=False):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessing():\n",
    "    \n",
    "    def __init__(self, erc20_names = ['ae','bat','fun','gno','gnt','loom','omg','rep','salt','snt','zrx']):\n",
    "        self.erc20_names = erc20_names # List of all erc20 tokens to be evaluated\n",
    "        self.raw_data = {} # raw DataFrames from csv\n",
    "        self.merged_data = {} # erc20 DataFrames with missing data removed & join with eth dataset on date\n",
    "        self.clean_data = {} # interpolated merged_data set to remove nan values (some first rows removed due to NaN values) \n",
    "        self.full_data = {}\n",
    "        self.final_sets = {}   \n",
    "        self.eval_sets = {}\n",
    "        \n",
    "        self.train_X = None\n",
    "        self.test_X = None\n",
    "        self.train_y = None\n",
    "        self.test_y = None\n",
    "        self.history = None\n",
    "        \n",
    "        self.IRRELEVANT_COLUMNS = ['date','eth_generatedCoins','eth_averageDifficulty','eth_blockCount','eth_percent_inrcease(USD)','current_performance_vs_eth','performance_vs_eth']\n",
    "\n",
    "        self.get_raw_datasets(self.erc20_names)\n",
    "        \n",
    "    def get_raw_datasets(self, erc20_names):\n",
    "        eth_dataset=read_csv('eth.csv', index_col=False)\n",
    "        rename_columns('eth', eth_dataset)\n",
    "        self.raw_data['eth'] = eth_dataset\n",
    "        for erc20 in erc20_names:\n",
    "            csv = erc20 + '.csv'            \n",
    "            dataset=read_csv(csv, index_col=False)\n",
    "            self.raw_data[erc20] = dataset\n",
    "            \n",
    "    def merge_data(self):\n",
    "        for erc20 in self.erc20_names:\n",
    "            dataset = self.raw_data[erc20].copy().fillna(0)\n",
    "            eth_dataset = self.raw_data['eth'].copy()\n",
    "            \n",
    "            #get the number of rows that have all valid data & slice datasets to only include this\n",
    "            valid_record_count = len(dataset) - get_shortest_feature(dataset) \n",
    "            sliced_data = dataset.iloc[valid_record_count:]\n",
    "            sliced_eth_data = slice_base_asset_data(eth_dataset,sliced_data,'eth_date')\n",
    "            \n",
    "            #sanity check sliced datasets are the same shape\n",
    "            print('sliced eth data shape {}'.format(sliced_eth_data.shape))\n",
    "            print('sliced {} data shape {}'.format(erc20, sliced_data.shape))\n",
    "            \n",
    "            #merge erc20 data & eth data (join on date)\n",
    "            merged_data = merge_frames(sliced_data,eth_dataset)\n",
    "            self.merged_data[erc20] = merged_data.copy()\n",
    "            \n",
    "    def remove_nans(self):\n",
    "        for erc20 in self.erc20_names:\n",
    "            merged_data = replace_bad_values(self.merged_data[erc20].copy())            \n",
    "            #Repeat Sanity Check for missing values\n",
    "            print('{} NaN Cells: {}'.format(erc20, merged_data.isnull().values.sum()))\n",
    "            self.clean_data[erc20] = merged_data\n",
    "            \n",
    "    def populate_infered_data(self):\n",
    "        for erc20 in self.erc20_names:\n",
    "            \n",
    "            dataset = self.clean_data[erc20].copy()\n",
    "            \n",
    "            #add nan filled columns\n",
    "            dataset.insert(5, 'price_percent_inrcease(USD)', np.nan, allow_duplicates=False)\n",
    "            dataset.insert(5, 'price_inrcease(USD)', np.nan, allow_duplicates=False)\n",
    "            dataset.insert(15, 'eth_price_percent_inrcease(USD)', np.nan, allow_duplicates=False)\n",
    "#             dataset.insert(15, 'eth_percent_inrcease(USD)', np.nan, allow_duplicates=False)\n",
    "#             dataset['performance_vs_eth'] = np.nan\n",
    "            \n",
    "            #generate difference from previous day's price\n",
    "            dataset['price_inrcease(USD)'] = dataset['price(USD)'].diff()\n",
    "            dataset['eth_price_inrcease(USD)'] = dataset['eth_price(USD)'].diff()\n",
    "\n",
    "            #generate percentage difference from previous day's price\n",
    "            dataset['price_percent_inrcease(USD)'] = dataset['price(USD)'].pct_change()\n",
    "            dataset['eth_price_percent_inrcease(USD)'] = dataset['eth_price(USD)'].pct_change()\n",
    "\n",
    "            #Fill labels column, uses the next days price data to infer the target for the current day\n",
    "            dataset['current_performance_vs_eth'] = generate_labels(dataset)\n",
    "            dataset['next_day_performance_vs_eth'] = dataset['current_performance_vs_eth'].shift(-1)\n",
    "            \n",
    "            #NOTE: WOULD BE COOL TO ADD A HEAT CHART FOR WHAT DAYS ALL ASSETS OUTPERFORMED ETH\n",
    "            print('{} outperformed ETH on {} / {} days'.format(erc20, dataset['current_performance_vs_eth'].sum(), dataset['current_performance_vs_eth'].count()))\n",
    "            print(dataset['current_performance_vs_eth'].sum() / dataset['current_performance_vs_eth'].count() * 100,  '% of the time')\n",
    "\n",
    "            self.full_data[erc20] = dataset[1:-1]\n",
    "            \n",
    "    def process_raw_dataset(self, csv_file, name):\n",
    "        #get Eth dataset\n",
    "        eth_dataset=read_csv('eth.csv', index_col=False)\n",
    "        rename_columns('eth', eth_dataset)\n",
    "        dataset=read_csv(csv_file, index_col=False)\n",
    "        dataset.fillna(0, inplace=True)\n",
    "\n",
    "        #plot graphs\n",
    "        plot_timeseries_graphs(dataset, name)\n",
    "        plot_timeseries_graphs(eth_dataset)\n",
    "\n",
    "        valid_record_count = len(dataset) - get_shortest_feature(dataset)\n",
    "        sliced_data = dataset.iloc[valid_record_count:]\n",
    "        sliced_eth_data = slice_base_asset_data(eth_dataset,sliced_data,'eth_date')\n",
    "\n",
    "        #sanity check sliced datasets are the same shape\n",
    "        print('sliced eth data shape {}'.format(sliced_eth_data.shape))\n",
    "        print('sliced {} data shape {}'.format(name, sliced_data.shape))\n",
    "        merged_data = merge_frames(sliced_data,eth_dataset)\n",
    "\n",
    "        #Plot Graphs\n",
    "        plot_timeseries_graphs(merged_data)\n",
    "\n",
    "        #remove 0's & NaN\n",
    "        merged_data = replace_bad_values(merged_data)\n",
    "\n",
    "        #Repeat Sanity Check for missing values\n",
    "        print('NaN Cells: {}'.format(merged_data.isnull().values.sum()))\n",
    "\n",
    "        #Repeat Sanity Check for 0 values\n",
    "        print('Cells With a 0: {}'.format(merged_data.isin([0]).sum().sum()))\n",
    "        merged_data.insert(5, 'price_percent_inrcease(USD)', np.nan, allow_duplicates=False)\n",
    "        merged_data.insert(5, 'price_inrcease(USD)', np.nan, allow_duplicates=False)\n",
    "        merged_data.insert(15, 'eth_price_percent_inrcease(USD)', np.nan, allow_duplicates=False)\n",
    "        merged_data.insert(15, 'eth_percent_inrcease(USD)', np.nan, allow_duplicates=False)\n",
    "        merged_data['performance_vs_eth'] = np.nan\n",
    "\n",
    "        #generate difference from previous day's price\n",
    "        merged_data['price_inrcease(USD)'] = merged_data['price(USD)'].diff()\n",
    "        merged_data['eth_price_inrcease(USD)'] = merged_data['eth_price(USD)'].diff()\n",
    "\n",
    "        #generate percentage difference from previous day's price\n",
    "        merged_data['price_percent_inrcease(USD)'] = merged_data['price(USD)'].pct_change()\n",
    "        merged_data['eth_price_percent_inrcease(USD)'] = merged_data['eth_price(USD)'].pct_change()\n",
    "\n",
    "        #Fill labels column, uses the next days price data to infer the target for the current day\n",
    "        merged_data['current_performance_vs_eth'] = generate_labels(merged_data)\n",
    "        merged_data['next_day_performance_vs_eth'] = merged_data['current_performance_vs_eth'].shift(-1)\n",
    "\n",
    "        print('{} outperformed ETH on {} / {} days'.format(name, merged_data['current_performance_vs_eth'].sum(), merged_data['current_performance_vs_eth'].count()))\n",
    "        print(merged_data['current_performance_vs_eth'].sum() / merged_data['current_performance_vs_eth'].count() * 100,  '% of the time')\n",
    "\n",
    "        #remove irrelevant columns\n",
    "        relevant_data = merged_data.drop(['date','eth_generatedCoins','eth_averageDifficulty','eth_blockCount','eth_percent_inrcease(USD)','current_performance_vs_eth','performance_vs_eth'], axis =1)\n",
    "\n",
    "        #remove the first row(due to missing data)\n",
    "        relevant_data = relevant_data.drop(0)\n",
    "        relevant_data = relevant_data.drop(1)\n",
    "\n",
    "        relevant_data = relevant_data[:-1]\n",
    "\n",
    "        # load dataset\n",
    "        new_dataset = relevant_data\n",
    "        values = new_dataset.values\n",
    "        # integer encode direction\n",
    "        encoder = LabelEncoder()\n",
    "        values[:,4] = encoder.fit_transform(values[:,4])\n",
    "        # ensure all data is float\n",
    "        values = values.astype('float32')\n",
    "\n",
    "        # normalize features\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        # frame as supervised learning\n",
    "        reframed = series_to_supervised(scaled, 1, 1)\n",
    "        # drop columns we don't want to predict\n",
    "        # reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\n",
    "        # print(len(reframed))\n",
    "        # print(len(relevant_data))\n",
    "        reframed = reframed.drop(0)\n",
    "        \n",
    "        self.final_sets[name] = reframed \n",
    "\n",
    "        return \"upload for {} complete\".format(csv_file)\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(40,input_shape=(self.train_X.shape[1], self.train_X.shape[2])))\n",
    "        # model.add(Flatten())\n",
    "        self.model.add(Dense(25))\n",
    "        self.model.add(Dropout(0.8))\n",
    "        self.model.add(Dense(25))\n",
    "        self.model.add(Dropout(0.8))\n",
    "\n",
    "        self.model.add(Dense(25))\n",
    "        self.model.add(Dropout(0.8))\n",
    "\n",
    "        self.model.add(Dense(25))\n",
    "\n",
    "        \n",
    "        self.model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            \n",
    "        return \"build complete\"\n",
    "    \n",
    "    #use self.final_sets.get(rep) as the input variable\n",
    "    def set_train_test_values(self, dataset, n_train_days=250):\n",
    "            values = dataset.values\n",
    "            train = values[:n_train_days, :]\n",
    "            test = values[n_train_days:, :]\n",
    "\n",
    "            # split into input and outputs\n",
    "            self.train_X, self.train_y = train[:, :-1], train[:, -1]\n",
    "            print(len(self.train_X), len(self.train_y))\n",
    "            self.test_X, self.test_y = test[:, :-1], test[:, -1]\n",
    "            print(len(self.test_X), len(self.test_y))\n",
    "\n",
    "            # reshape input to be 3D [samples, timesteps, features]\n",
    "            self.train_X = self.train_X.reshape((self.train_X.shape[0], 1, self.train_X.shape[1]))\n",
    "            self.test_X = self.test_X.reshape((self.test_X.shape[0], 1, self.test_X.shape[1]))\n",
    "            print(self.train_X.shape, self.train_y.shape, self.test_X.shape, self.test_y.shape)\n",
    "            \n",
    "    def set_eval_values(self, test_days=None):\n",
    "        for erc20 in self.final_sets:\n",
    "            \n",
    "            values = self.final_sets.get(erc20).values\n",
    "            days = len(values)\n",
    "            if test_days == None:\n",
    "                test_days = len(self.test_y)\n",
    "        \n",
    "            n_train_days = days - test_days \n",
    "            train = values[:n_train_days, :]\n",
    "            test = values[n_train_days:, :]\n",
    "\n",
    "            # split into input and outputs\n",
    "            train_X, train_y = train[:, :-1], train[:, -1]\n",
    "            print(len(train_X), len(train_y))\n",
    "            test_X, test_y = test[:, :-1], test[:, -1]\n",
    "            print(len(test_X), len(test_y))\n",
    "\n",
    "            # reshape input to be 3D [samples, timesteps, features]\n",
    "            self.eval_sets[erc20] = {'test_X' : test_X.reshape((test_X.shape[0], 1, test_X.shape[1])), 'test_y' : test_y} \n",
    "        \n",
    "    def fit_model(self):        \n",
    "        self.history = self.model.fit(self.train_X, self.train_y, epochs=400, batch_size=50, validation_data=(self.test_X, self.test_y), verbose=2, shuffle=False)\n",
    "        # plot history\n",
    "        pyplot.plot(self.history.history['loss'], label='train')\n",
    "        pyplot.plot(self.history.history['val_loss'], label='test')\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "\n",
    "        pyplot.plot(self.history.history['acc'], label='train')\n",
    "        pyplot.plot(self.history.history['val_acc'], label='test')\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataProcessing = DataProcessing(erc20_names=['ae','bat','fun','gno','gnt','rep'])\n",
    "DataProcessing = DataProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sliced eth data shape (312, 16)\n",
      "sliced ae data shape (312, 8)\n",
      "sliced eth data shape (411, 16)\n",
      "sliced bat data shape (411, 8)\n",
      "sliced eth data shape (369, 16)\n",
      "sliced fun data shape (369, 8)\n",
      "sliced eth data shape (441, 16)\n",
      "sliced gno data shape (441, 8)\n",
      "sliced eth data shape (595, 16)\n",
      "sliced gnt data shape (595, 8)\n",
      "sliced eth data shape (119, 16)\n",
      "sliced loom data shape (119, 8)\n",
      "sliced eth data shape (367, 16)\n",
      "sliced omg data shape (367, 8)\n",
      "sliced eth data shape (346, 16)\n",
      "sliced rep data shape (346, 8)\n",
      "sliced eth data shape (292, 16)\n",
      "sliced salt data shape (292, 8)\n",
      "sliced eth data shape (384, 16)\n",
      "sliced snt data shape (384, 8)\n",
      "sliced eth data shape (336, 16)\n",
      "sliced zrx data shape (336, 8)\n"
     ]
    }
   ],
   "source": [
    "DataProcessing.merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae NaN Cells: 0\n",
      "bat NaN Cells: 0\n",
      "fun NaN Cells: 0\n",
      "gno NaN Cells: 0\n",
      "gnt NaN Cells: 0\n",
      "loom NaN Cells: 0\n",
      "omg NaN Cells: 0\n",
      "rep NaN Cells: 0\n",
      "salt NaN Cells: 0\n",
      "snt NaN Cells: 0\n",
      "zrx NaN Cells: 0\n"
     ]
    }
   ],
   "source": [
    "DataProcessing.remove_nans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae outperformed ETH on 145 / 312 days\n",
      "46.47435897435898 % of the time\n",
      "bat outperformed ETH on 190 / 411 days\n",
      "46.228710462287104 % of the time\n",
      "fun outperformed ETH on 160 / 368 days\n",
      "43.47826086956522 % of the time\n",
      "gno outperformed ETH on 183 / 441 days\n",
      "41.49659863945578 % of the time\n",
      "gnt outperformed ETH on 257 / 595 days\n",
      "43.19327731092437 % of the time\n",
      "loom outperformed ETH on 53 / 119 days\n",
      "44.537815126050425 % of the time\n",
      "omg outperformed ETH on 161 / 367 days\n",
      "43.869209809264305 % of the time\n",
      "rep outperformed ETH on 155 / 346 days\n",
      "44.797687861271676 % of the time\n",
      "salt outperformed ETH on 120 / 292 days\n",
      "41.0958904109589 % of the time\n",
      "snt outperformed ETH on 158 / 384 days\n",
      "41.14583333333333 % of the time\n",
      "zrx outperformed ETH on 152 / 336 days\n",
      "45.23809523809524 % of the time\n"
     ]
    }
   ],
   "source": [
    "DataProcessing.populate_infered_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                                object\n",
       "txVolume(USD)                      float64\n",
       "txCount                            float64\n",
       "marketcap(USD)                     float64\n",
       "price(USD)                         float64\n",
       "price_inrcease(USD)                float64\n",
       "price_percent_inrcease(USD)        float64\n",
       "exchangeVolume(USD)                float64\n",
       "activeAddresses                    float64\n",
       "medianTxValue(USD)                 float64\n",
       "eth_txVolume(USD)                  float64\n",
       "eth_adjustedTxVolume(USD)          float64\n",
       "eth_txCount                        float64\n",
       "eth_marketcap(USD)                 float64\n",
       "eth_price(USD)                     float64\n",
       "eth_price_percent_inrcease(USD)    float64\n",
       "eth_exchangeVolume(USD)            float64\n",
       "eth_generatedCoins                 float64\n",
       "eth_fees                           float64\n",
       "eth_activeAddresses                float64\n",
       "eth_medianTxValue(USD)             float64\n",
       "eth_medianFee                      float64\n",
       "eth_averageDifficulty              float64\n",
       "eth_paymentCount                   float64\n",
       "eth_blockSize                        int64\n",
       "eth_blockCount                       int64\n",
       "eth_price_inrcease(USD)            float64\n",
       "current_performance_vs_eth           int64\n",
       "next_day_performance_vs_eth        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataProcessing.full_data['ae'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_TO_IGNORE = ['date','current_performance_vs_eth','eth_generatedCoins']\n",
    "\n",
    "values = DataProcessing.full_data['ae'].copy().drop('date',axis=1)\n",
    "dates = DataProcessing.full_data['ae'].copy()['date']        \n",
    "values = values.astype('float32')\n",
    "\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(series_to_supervised(scaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame()\n",
    "labels['date'] = DataProcessing.full_data['gnt']['date']\n",
    "labels['sum'] = np.nan\n",
    "\n",
    "for key in DataProcessing.erc20_names:\n",
    "#     labels[key + '_y'] = DataProcessing.full_data[key]['next_day_performance_vs_eth']\n",
    "    labels = pd.merge(labels, DataProcessing.full_data[key][['date','next_day_performance_vs_eth']],how='outer', left_on='date', right_on='date',suffixes=(\"\", \"_\" +key))\n",
    "#     labels[key + 'test'] = np.where(labels['date'] == DataProcessing.full_data[key]['date'], DataProcessing.full_data[key]['next_day_performance_vs_eth'], np.nan)\n",
    "#     labels[key + 'test'].map(DataProcessing.full_data[key]['next_day_performance_vs_eth'])\n",
    "\n",
    "labels[:-1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.defs.vis as vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.sns_labels_heatmap(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plotly_labels_heatmap(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProcessing.set_train_test_values(DataProcessing.final_sets.get('rep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_day_performance_vs_eth_omg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProcessing.set_eval_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProcessing.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProcessing.test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DataProcessing.merged_data['fun'].loc[0].isnull().sum() > 0:\n",
    "    print (\"yes\")\n",
    "else:\n",
    "    print (\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_bad_valuess(dataset):\n",
    "    dataset = dataset.copy().replace(0,np.nan)\n",
    "    if dataset.loc[0].isnull().sum() > 0:\n",
    "        dataset.drop(0, inplace=True)\n",
    "        dataset.reset_index(inplace=True)\n",
    "    return dataset.interpolate()\n",
    "\n",
    "replace_bad_valuess(DataProcessing.merged_data['fun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
